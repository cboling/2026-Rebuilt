# ------------------------------------------------------------------------ #
#      o-o      o                o                                         #
#     /         |                |                                         #
#    O     o  o O-o  o-o o-o     |  oo o--o o-o o-o                        #
#     \    |  | |  | |-' |   \   o | | |  |  /   /                         #
#      o-o o--O o-o  o-o o    o-o  o-o-o--O o-o o-o                        #
#             |                           |                                #
#          o--o                        o--o                                #
#                        o--o      o         o                             #
#                        |   |     |         |  o                          #
#                        O-Oo  o-o O-o  o-o -o-    o-o o-o                 #
#                        |  \  | | |  | | |  |  | |     \                  #
#                        o   o o-o o-o  o-o  o  |  o-o o-o                 #
#                                                                          #
#    Jemison High School - Huntsville Alabama                              #
# ------------------------------------------------------------------------ #

try:
    import logging

    from typing import List, Optional

    from ntcore import NetworkTableInstance
    from robotpy_apriltag import AprilTagField, AprilTagFieldLayout
    from wpimath.geometry import Transform3d, Rotation2d, Pose3d
    from wpimath.units import milliseconds, seconds, meters

    from lib_6107.subsystems.vision.visionsubsystem import VisionSubsystem, VisionTargetData, VisionConsumer
    from lib_6107.util.field import Field
    from lib_6107.subsystems.pykit.vision_io import VisionIO, TargetObservation, \
        PoseObservation, PoseObservationType

    from photonlibpy import PhotonCamera, PhotonPoseEstimator
    from photonlibpy.targeting.photonPipelineResult import PhotonPipelineResult, PhotonTrackedTarget, \
        MultiTargetPNPResult

    logger = logging.getLogger(__name__)


    class PhotonVisionSubsystem(VisionSubsystem):
        def __init__(self, vision_input: VisionConsumer,
                     name: str, field: Field, transform: Transform3d, drivetrain: 'DriveSubsystem'):
            super().__init__(vision_input, name, field, transform, drivetrain)

            self._camera: PhotonCamera = PhotonCamera(name)
            self._latest_results: Optional[PhotonPipelineResult] = None

            # self._sim_camera = TODO: More work here

            # In simulation, load the field layout
            # TODO: Is there a way to query existing field layout so we can possibly skip this
            #       step?
            self._estimator: PhotonPoseEstimator = PhotonPoseEstimator(self._field_layout,
                                                                       self._camera_transform)
            # Register for field layout changes
            field.register_layout_callback(self._on_field_change)

        def _on_field_change(self, _field: AprilTagField, layout: AprilTagFieldLayout) -> None:
            """
            Operator selected a different field layout.
            """
            self._estimator.fieldTags = layout

        @property
        def latency(self) -> Optional[milliseconds]:
            results = self._latest_results or self._get_latest_results()

            return results.getLatencyMillis() if results is not None else None

        @property
        def timestamp(self) -> Optional[seconds]:
            """
            Returns the estimated time the frame was taken, in the Received system's time base
            """
            results = self._latest_results or self._get_latest_results()

            return results.getTimestampSeconds() if results is not None else None

        @property
        def best_target(self) -> Optional[VisionTargetData]:
            """
            Returns the best target in this pipeline result. If there are no targets, this method will
            return null. The best target is determined by the target sort mode in the PhotonVision UI.
            """
            results = self._latest_results or self._get_latest_results()

            photon_target: PhotonTrackedTarget = results.getBestTarget() if results is not None else None
            if photon_target is None:
                return None

            return VisionTargetData(photon_target.yaw, photon_target.pitch, photon_target.area,
                                    photon_target.fiducialId, photon_target.poseAmbiguity,
                                    photon_target.bestCameraToTarget, photon_target.altCameraToTarget)

        def _get_latest_results(self) -> Optional[PhotonPipelineResult]:
            self._latest_results = self._camera.getLatestResult()
            return self._latest_results

        def periodic(self) -> None:
            super().periodic()

            if not self._is_simulation and self._estimator is not None:
                latest_result: Optional[PhotonPipelineResult] = self.get_latest_results()

                if latest_result is not None and latest_result.hasTargets:
                    self._estimator.TODO("WHAT TO DO HERE?")

                # Clear latest_results so we will get new results on the next pass
                self._latest_results = None

        def simulationPeriodic(self):
            super().simulationPeriodic()

            # Update simulation based on physics engine (e.g., swerve drive sim)
            sim_robot_pose = self._drivetrain.pose

            # TODO: PhotonVision has quite a few things to support simulation...
            # Simulate camera seeing tags based on current pose
            # (This requires using VisionSystemSim in more complex setups)
            # ...

            # Update estimator with simulated data
            # TODO: elf._estimator.update()

            # Clear latest_results so we will get new results on the next pass
            self._latest_results = None

        def updateInputs(self, inputs: VisionIO.VisionIOInputs) -> None:
            """
            Pykit support for AdvantageScope
            """
            inputs.connected = self._camera.isConnected()

            # Read new camera observations
            # TODO: In java version of this, this is a set. We want a set but we need to keep order perhaps?
            inputs.tag_ids = []
            inputs.pose_observations = []

            for result in self._camera.getAllUnreadResults():
                # Update latest target observation

                if result is not None and result.hasTargets():
                    inputs.latest_target_observation = \
                        TargetObservation(Rotation2d.fromDegrees(result.getBestTarget().getYaw()),
                                          Rotation2d.fromDegrees(result.getBestTarget().getPitch()), )
                else:
                    inputs.latest_target_observation = TargetObservation(Rotation2d(0),
                                                                         Rotation2d(0))
                # Add pose observation
                multi_tag_result: MultiTargetPNPResult = result.multitagResult
                single_target: PhotonTrackedTarget = result.getBestTarget()

                if multi_tag_result is not None:
                    # Multi-tag result processing
                    # Calculate robot pose
                    field_to_camera: Transform3d = multi_tag_result.estimatedPose.best
                    field_to_robot: Transform3d = field_to_camera + self._camera_transform.inverse()
                    robot_pose: Pose3d = Pose3d(field_to_robot.translation(), field_to_robot.rotation())

                    # Calculate average tag distance
                    total_tag_distance: meters = sum(target.bestCameraToTarget.translation().norm()
                                                     for target in result.targets)
                    # Add tag IDs
                    inputs.tag_ids = multi_tag_result.fiducialIDsUsed

                    # Add observation
                    inputs.pose_observations.append(
                        PoseObservation(result.getTimestampSeconds(),
                                        robot_pose,
                                        multi_tag_result.estimatedPose.ambiguity,
                                        len(multi_tag_result.fiducialIDsUsed),
                                        total_tag_distance / len(result.targets),
                                        PoseObservationType.PHOTONVISION))

                elif single_target is not None:
                    # Single target acquired. Note this is also the 'best' if it was multi-target but
                    # that is handled in the previous 'if' clause
                    #
                    # Calculate robot pose

                    tag_pose = self._field_layout.getTagPose(single_target.fiducialId)
                    if tag_pose:
                        field_to_target: Transform3d = Transform3d(tag_pose.translation(), tag_pose.rotation())
                        camera_to_target: Transform3d = single_target.bestCameraToTarget
                        field_to_camera: Transform3d = field_to_target + camera_to_target.inverse()
                        field_to_robot: Transform3d = field_to_camera.plus(self._camera_transform.inverse())

                        robot_pose: Pose3d = Pose3d(field_to_robot.translation(), field_to_robot.rRotation())

                        # Add tag ID
                        inputs.tag_ids = [single_target.fiducialId]

                        # Add observation
                        inputs.pose_observations = [PoseObservation(result.getTimestampSeconds(),
                                                                    robot_pose,  # 3D pose estimate
                                                                    single_target.poseAmbiguity,  # Ambiguity
                                                                    1,  # Tag count
                                                                    camera_to_target.translation().norm(),
                                                                    # Average tag distance
                                                                    PoseObservationType.PHOTONVISION)]

except ImportError as _e:
    raise
